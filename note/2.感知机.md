# 感知机

感知机(perceptron) 是***二类分类的线性分类模型***，**其输入为实例的特征向量输出为实例的类别，取+1 和一1 二值**。

感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面，属于判别模型。**感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。**

感知机学习算法具有简单而易于实现的优点，分为**原始形式**和**对偶形式**。感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机1957 年由Rosenblatt 提出，**是神经网络与支持向量机的基础**。

## 1. 感知机模型

### 感知机定义

假设输入空间(特征空间)是$\mathcal{X}\subseteq R^n$输出空间是$\mathcal{Y}=\{+1,-1\}$。输入$x\in\mathcal{X}$表示实例得特征向量，对应输入空间的点；输出$y\in\mathcal{\mathcal{Y}}$表示实例的类别。输入空间到输出空间的如下函数：

$$f(x)=\mathbf{sign}(w ·x+b)$$

上式便称之为感知机，其中，$w$和$b$称之为感知机模型参数，$w\in R^n$叫做权值或者权值向量，$b\in R$叫做偏置，$w·x$表示它们作内积，sign是符号函数即：
$$
\mathbf{sign}(x)=
\begin{cases}
     +1,\quad x\geq 0 \\
    -1,\quad x<0
\end{cases}
$$

---

**感知机是一种线性分类模型，属于判别模型。** 感知机模型的假设空间是定义在特征空间中的所有线性分类模型(linear classification model)或线性分类器(linear classifier),即函数集合$\{f|f(x)=w·x+b\}$

感知机有如下几何解释：线性方程

$$w·x+b=0$$

对应于特征空间$R^n$中的超平面$S$,其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点分别呗分为正负两类。因此，超平面$S$称为分离超平面，如下图所示

<center><img src="https://z3.ax1x.com/2021/07/15/WmNiUf.jpg" alt="WmNiUf.jpg" border="0" /></center>

---
感知机学习由训练集（示例的特征向量及类别）
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$

其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。感知机模型需要求得模型参数$w,b$。感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。

---

## 2. 感知机的学习策略

### 2.1数据集的线性可分性

给定一个数据集
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$
其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$,如果存在一个超平面$S$

$$w·x+b=0$$

能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对于所有$y_i=+1$的实例$i$,都有$w·x_i+b > 0$,对于所有$y_i=-1$的实例$i$,都有$w·x_i+b < 0$，则称数据集$T$为线性可分数据集，否则称$T$线性不可分

---

### 2.2 感知机学习策略

假设训练数据集是线性可分的，**感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面**。为了找出这样的超平面，即确定感知机模型参数$w,b$ ， **需要确定一个学习策略，即定义(经验)损失函数并将损失函数极小化。**

#### 感知机的损失函数

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数$w,b$的连续可导函数，不易优化。
**损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的**

为此，首先写出输入空间$R^n$中任意一点$x_0$到超平面$S$的距离

$$\frac{1}{\|w\|}|w·x_0+b|$$

$\|w\|是w$的$L_2$范数
其次对于误分类的数据$(x_i,y_i)$来说，
$$-y_i(w·x_i+b)>0$$
成立。因为当$w·x_i+b>0$时，$y_i = -1$,反之，当$w·x_i+b<0$时$y_i = +1$。因此误差分类点$x_i$到超平面$S$的距离是
$$-\frac{1}{\|w\|}y_i|w·x_i+b|$$

这样，假设超平面$S$的误分类点集合为$M$， 那么所有误分类点到超平面$S$ 的总距离为

$$-\frac{1}{\|w\|}\sum_{x_i\in M}y_i(w·x_i+b)$$
不考虑$\frac{1}{\|w\|}$就得到感知机的损失函数

对于给定的数据集$T$，感知机$\mathbf{sign}(w·x+b)$学习的损失函数定义为
$$L(w,b) = - \sum_{x_i\in M}y_i(w·x_i+b)$$
其中$M$为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。

显然，损失函数的是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。一个特定的样本点的损失函数:在误分类时是参数的线性函数，在正确分类时是0。因此，给定训练数据集$T$ ， 损失函数$L(w,b)是w , b$的连续可导函数。

感知机学习的策略是在假设空间中选取使损失函数最小的模型参数$w , b$即感知机模型。

---

## 3. 感知机学习算法

感知机学习问题转化为求解损失函数的最优化问题，**最优化的方法是随机梯度下降法**。本节叙述感知机学习的具体算法，**包括原始形式和对偶形式**，并证明在训练数据线性可分条件下感知机学习算法的收敛性。

### 3.1 感知机学习算法的原始形式

给定一个数据集
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$
其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$,求参数$w,b$使其为一下损失函数极小化问题的解

$$\min_{w,b}L(w,b)=-\sum_{x_i\in M}y_i(w·x_i+b)$$
其中$M$为误分类点的集合

感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首先，任意选取一个超平面$(w_0,b_0)$ 然后用梯度下降法不断地极小化目标函数。及鲜花过程中不是一次使$M$中所有的误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由

$$
\begin{aligned}
    \nabla_wL(w,b)&=-\sum_{x_i\in M}y_ix_i\\
    \nabla_bL(w,b)&=-\sum_{x_i\in M}y_i
\end{aligned}
$$
随机选取一个误分类点$(x_i,y_i)$,对于$w,b$进行更新
$$
\begin{aligned}
    w &\larr w+\eta y_ix_i \\
    b&\larr b + \eta y_i
\end{aligned}
$$

其中$\eta$是步长，又称之为学习率，通过迭代就可以使得损失函数$L(w,b)$不断减小，直到为0.综上可以得到如下算法

#### 感知机学习算法的原始形式

**输入：**训练集$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$；学习率$\eta (0<\eta<1)$

**输出：**$w,b;$感知机模型$f(x)=\mathbf{sign}(w·x+b)$.

1. 选取初始值$w_0,b_0$;
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i(w·x_i+b) \leq 0$（误分类）,
   $$
   \begin{aligned}
       w &\larr w+\eta y_ix_i \\
       b &\larr b + \eta y_i
   \end{aligned}
   $$
4. 转至$2$直到训练集中没有误分类点

这种学习算法直观上有如下解释:当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。