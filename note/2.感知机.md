# 感知机

感知机(perceptron) 是***二类分类的线性分类模型***，**其输入为实例的特征向量输出为实例的类别，取+1 和一1 二值**。

感知机对应于输入空间(特征空间)中将实例划分为正负两类的分离超平面，属于判别模型。**感知机学习旨在求出将训练数据进行线性划分的分离超平面，为此，导入基于误分类的损失函数，利用梯度下降法对损失函数进行极小化，求得感知机模型。**

感知机学习算法具有简单而易于实现的优点，分为**原始形式**和**对偶形式**。感知机预测是用学习得到的感知机模型对新的输入实例进行分类。感知机1957 年由Rosenblatt 提出，**是神经网络与支持向量机的基础**。

## 1. 感知机模型

### 感知机定义

假设输入空间(特征空间)是$\mathcal{X}\subseteq R^n$输出空间是$\mathcal{Y}=\{+1,-1\}$。输入$x\in\mathcal{X}$表示实例得特征向量，对应输入空间的点；输出$y\in\mathcal{\mathcal{Y}}$表示实例的类别。输入空间到输出空间的如下函数：

$$f(x)=\mathbf{sign}(w ·x+b)$$

上式便称之为感知机，其中，$w$和$b$称之为感知机模型参数，$w\in R^n$叫做权值或者权值向量，$b\in R$叫做偏置，$w·x$表示它们作内积，sign是符号函数即：
$$
\mathbf{sign}(x)=
\begin{cases}
     +1,\quad x\geq 0 \\
    -1,\quad x<0
\end{cases}
$$

---

**感知机是一种线性分类模型，属于判别模型。** 感知机模型的假设空间是定义在特征空间中的所有线性分类模型(linear classification model)或线性分类器(linear classifier),即函数集合$\{f|f(x)=w·x+b\}$

感知机有如下几何解释：线性方程

$$w·x+b=0$$

对应于特征空间$R^n$中的超平面$S$,其中$w$是超平面的法向量，$b$是超平面的截距。这个超平面将特征空间划分为两个部分。位于两部分的点分别呗分为正负两类。因此，超平面$S$称为分离超平面，如下图所示

<center><img src="https://z3.ax1x.com/2021/07/15/WmNiUf.jpg" alt="WmNiUf.jpg" border="0" /></center>

---
感知机学习由训练集（示例的特征向量及类别）
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$

其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$。感知机模型需要求得模型参数$w,b$。感知机预测，通过学习得到的感知机模型，对于新的输入实例给出其对应的输出类别。

---

## 2. 感知机的学习策略

### 2.1数据集的线性可分性

给定一个数据集
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$
其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$,如果存在一个超平面$S$

$$w·x+b=0$$

能够将数据集的正实例点和负实例点完全正确的划分到超平面的两侧，即对于所有$y_i=+1$的实例$i$,都有$w·x_i+b > 0$,对于所有$y_i=-1$的实例$i$,都有$w·x_i+b < 0$，则称数据集$T$为线性可分数据集，否则称$T$线性不可分

---

### 2.2 感知机学习策略

假设训练数据集是线性可分的，**感知机学习的目标是求得一个能够将训练集正实例点和负实例点完全正确分开的分离超平面**。为了找出这样的超平面，即确定感知机模型参数$w,b$ ， **需要确定一个学习策略，即定义(经验)损失函数并将损失函数极小化。**

#### 感知机的损失函数

损失函数的一个自然选择是误分类点的总数。但是，这样的损失函数不是参数$w,b$的连续可导函数，不易优化。
**损失函数的另一个选择是误分类点到超平面S的总距离，这是感知机所采用的**

为此，首先写出输入空间$R^n$中任意一点$x_0$到超平面$S$的距离

$$\frac{1}{\|w\|}|w·x_0+b|$$

$\|w\|是w$的$L_2$范数
其次对于误分类的数据$(x_i,y_i)$来说，
$$-y_i(w·x_i+b)>0$$
成立。因为当$w·x_i+b>0$时，$y_i = -1$,反之，当$w·x_i+b<0$时$y_i = +1$。因此误差分类点$x_i$到超平面$S$的距离是
$$-\frac{1}{\|w\|}y_i|w·x_i+b|$$

这样，假设超平面$S$的误分类点集合为$M$， 那么所有误分类点到超平面$S$ 的总距离为

$$-\frac{1}{\|w\|}\sum_{x_i\in M}y_i(w·x_i+b)$$
不考虑$\frac{1}{\|w\|}$就得到感知机的损失函数

对于给定的数据集$T$，感知机$\mathbf{sign}(w·x+b)$学习的损失函数定义为
$$L(w,b) = - \sum_{x_i\in M}y_i(w·x_i+b)$$
其中$M$为误分类点的集合。这个损失函数就是感知机学习的经验风险函数。

显然，损失函数的是非负的。如果没有误分类点，损失函数值是0。而且，误分类点越少，误分类点离超平面越近，损失函数值就越小。一个特定的样本点的损失函数:在误分类时是参数的线性函数，在正确分类时是0。因此，给定训练数据集$T$ ， 损失函数$L(w,b)是w , b$的连续可导函数。

感知机学习的策略是在假设空间中选取使损失函数最小的模型参数$w , b$即感知机模型。

---

## 3. 感知机学习算法

感知机学习问题转化为求解损失函数的最优化问题，**最优化的方法是随机梯度下降法**。本节叙述感知机学习的具体算法，**包括原始形式和对偶形式**，并证明在训练数据线性可分条件下感知机学习算法的收敛性。

### 3.1 感知机学习算法的原始形式

给定一个数据集
$$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$$
其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$,求参数$w,b$使其为一下损失函数极小化问题的解

$$\min_{w,b}L(w,b)=-\sum_{x_i\in M}y_i(w·x_i+b)$$
其中$M$为误分类点的集合

感知机学习算法是误分类驱动的，具体采用随机梯度下降法。首先，任意选取一个超平面$(w_0,b_0)$ 然后用梯度下降法不断地极小化目标函数。及鲜花过程中不是一次使$M$中所有的误分类点的梯度下降，而是一次随机选取一个误分类点使其梯度下降。

假设误分类点集合$M$是固定的，那么损失函数$L(w,b)$的梯度由

$$
\begin{aligned}
    \nabla_wL(w,b)&=-\sum_{x_i\in M}y_ix_i\\
    \nabla_bL(w,b)&=-\sum_{x_i\in M}y_i
\end{aligned}
$$
随机选取一个误分类点$(x_i,y_i)$,对于$w,b$进行更新
$$
\begin{aligned}
    w &\larr w+\eta y_ix_i \\
    b&\larr b + \eta y_i
\end{aligned}
$$

其中$\eta$是步长，又称之为学习率，通过迭代就可以使得损失函数$L(w,b)$不断减小，直到为0.综上可以得到如下算法

#### 感知机学习算法的原始形式

**输入：** 训练集$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$；学习率$\eta (0<\eta<1)$

**输出：**$w,b;$感知机模型$f(x)=\mathbf{sign}(w·x+b)$.

1. 选取初始值$w_0,b_0$;
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i(w·x_i+b) \leq 0$（误分类）,
   $$
   \begin{aligned}
       w &\larr w+\eta y_ix_i \\
       b &\larr b + \eta y_i
   \end{aligned}
   $$
4. 转至$2$直到训练集中没有误分类点

这种学习算法直观上有如下解释:当一个实例点被误分类，即位于分离超平面的错误一侧时，则调整$w,b$的值，使分离超平面向该误分类点的一侧移动，以减少该误分类点与超平面间的距离，直至超平面越过该误分类点使其被正确分类。

### 3.2 算法的收敛性

> L2范数是指向量各元素的平方和然后求平方根

为了方便推导，将偏置$b$并入权重向量$w$,记作$\^w=(w^T,b)^T$,同样的将输入相机加以扩充，加进常熟1，记作$\^x=(x^T,1)^T$这样$\^x\in R^{n+1},\^w\in R^{n+1}$，显然$\^w·\^x=w·x+b$

> 利用Novikoff不等式证明感知机的收敛性

设训练数据集$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$则

1. 存在满足条件$\|\^w_{opt}\|$ = 1的超平面$\^w_{opt}·\^x=w_{opt}·x+b_{opt}=0$将训练数据集完全正确分离；且存在$\gamma>0$对所有$i=1,2,3,...,N$
$$y_i(\^w_{opt}·\^x_i) =y_i(w_{opt}·x_i+b_{opt}) \geq \gamma$$

2. 令$R=\underset{1\leq i\leq N}{\max}\|\^x_i\|$，则感知机算法在训练集上误分类的次数k满足不等式
   $$k \leq \left(\frac{R}{\gamma}\right)^2$$

#### **证明**

1. 由于训练数据集是线性可分的，按照感知机定义， 存在超平面可将训练数据集完全正确分开，取此超平面$w_{opt}·\^x=w_{opt}·x+b_{opt}=0$为使$\|\^w\|=1$ 。由于对有限的$i = 1,2,...,N$均有
   $$y_i(\^w_{opt}·\^x_i) =y_i(w_{opt}·x_i+b_{opt}) > 0$$
所以存在
    $$\gamma = \min_i\{y_i(w_{opt}·x_i+b_{opt})\}$$
使得
$$y_i(\^w_{opt}·\^x_i) =y_i(w_{opt}·x_i+b_{opt})\geq \gamma$$

2. 感知机算法从$\^w_0=0$开始，如果实例被误分类，则更新权重。令$\^w_{k-1}$是第$k$个误分类实例之前的扩充权值向量
   $$\^w_{k-1}=(w_{k-1}^T,b_{k-1})^T$$
则第$k$个误分类的实例条件为
$$y_i(\^w_{k-1}·\^x_i)=y_i(w_{k-1}·x_i+b_{k-1}) \leq 0$$
若$(x_i,y_i)$是被$\^w_{k-1}$误分类的数据则$w,b$的更新为
$$
\begin{aligned}
    w_k &\larr w_{k-1}+ \eta y_ix_i \\
    b_k &\larr b_{k-1} + \eta y_i
\end{aligned}
$$
即
$$\^w_k = \^w_{k-1}+\eta y_i\^x_i$$

由上式得
$$
\begin{aligned}
    &\^w_k ·\^w_{opt} = \^w_{k-1}·\^w_{opt} +\eta y_i\^w_{opt}·\^x_i \\
    &\because y_i(\^w_{opt}·\^x_i) \geq \gamma \\
    &\therefore \^w_{k-1}·\^w_{opt} +\eta y_i\^w_{opt}·\^x_i \geq \^w_{k-1}·\^w_{opt} +\eta \gamma \\
    &\therefore \^w_k ·\^w_{opt} \geq \^w_{k-1}·\^w_{opt} +\eta \gamma
\end{aligned}
$$
递推得
$$
\begin{aligned}
        \^w_k ·\^w_{opt} \geq \^w_{k-1}·\^w_{opt} +\eta \gamma \geq \^w_{k-2}·\^w_{opt} +2\eta \gamma\geq ...\geq k\eta\gamma
\end{aligned}
$$

由$\^w_k = \^w_{k-1}+\eta y_i\^x_i$得

$$
\begin{aligned}
    &\|\^w_k\|^2 = \|\^w_{k-1}\|^2+2\eta y_i\^w_{k-1}·\^x_i+\eta^2\|\^x_i\|^2 \\
    &\because 在这里是是误分类的，所以y_i\^w_{k-1}·\^x_i是小于0的 \\
    &又\because R=\underset{1\leq i\leq N}{\max}\|\^x_i\| \\
    &\therefore \|\^w_{k-1}\|^2+2\eta y_i\^w_{k-1}·\^x_i+\eta^2\|\^x_i\|^2 \\
    & \leq  \|\^w_{k-1}\|^2+\eta^2 R^2 \\
    &\leq \|\^w_{k-2}\|^2+2\eta^2 R^2 \leq ...\\
    &\leq k\eta^2R^2
\end{aligned}
$$

根据柯西不等式$|a·b|\leq |a||b|$得
$$k\eta\gamma \leq \^w_k\^w-{opt}\leq\|\^w_k\|\|\^w_{opt}\| \leq \sqrt{k}\eta R$$
于是有
$$
\begin{aligned}
    k^2\gamma^2 \leq k R^2 \\
    k \leq \left(\frac{R}{\gamma}\right)^2
\end{aligned}
$$

定理表明，误分类的次数$k$是有上界的，经过有限次搜索可以找到将训练数据完全正确分开的分离超平面。也就是说，当训练数据集线性可分时，感知机学习算法原始形式迭代是收敛的。但是例说明，感知机学习算法存在许多解，这些解既依赖于初值的选择，也依赖于迭代过程中误分类点的选择顺序。为了得到唯一的超平面，需要对分离超平面增加约束条件。这是线性支持向量机的想法。当训练集线性不可分时，感知机学习算法不收敛， 迭代结果会发生震荡。

### 3.3 感知机算法的对偶形式

现在考虑感知机学习算法的对偶形式。感知机学习算法的原始形式和对偶形式与支持向量机学习算法的原始形式和对偶形式相对应。

对偶形式的基本想法是，将$w,b$表示为实例$x_i$和标记$y_i$的线性组合，通过求解其系数而求得$w,b$。通过逐步修改$w,b$，设修改$n$次，则$w,b$关于$(x_i,y_i)$的增量分别是$\alpha_iy_ix_i和\alpha_iy_i$这里$\alpha_i=n_i\eta$这样，从学习过程不难看出，最后学习到的$w,b$可以分别表示为

$$
\begin{aligned}
    w&=\sum_{i=1}^N\alpha_i y_ix_i \\
    b&=\sum_{i=1}^N\alpha_i y_i
\end{aligned}
$$

在这里$\alpha \geq 0, i=1,2,...,N当\eta=1时$表示第$i$个实例点由于误分类而进行更新的次数。实例点更新的次数越多，意味着它距离分离超平面越近，也就越难正确分类。

对偶形式的基本想法是，将$w$和$b$表示为实例$x_i$和标记$y_i$ 的线性组合的形式，通过求解其系数而求得$w$ 和$b$。

#### 感知机学习算法的对偶形式

输入： 训练集$T=\{(x_1, y_1), (x_2,y_2),...,(x_N,y_N)\}$，其中$x_i \in \mathcal{X}=R^n, y_i \in \mathcal{Y}=\{+1,-1\},i=1,2,...,N$；学习率$\eta (0<\eta<1)$
输出： $\alpha,b$;感知机模型$f(x) = \mathbf{sign}(\sum_{j=1}^N\alpha_jy_jx_j·x+b)$,其中$\alpha=(\alpha_1,\alpha_2,.....,\alpha_N)^T$

1. $\alpha \larr 0, b \larr 0$
2. 在训练集中选取数据$(x_i,y_i)$
3. 如果$y_i\left(\sum_{j=1}^N\alpha_jy_jx_j·x+b\right) \leq 0$
    $$\alpha_i \larr \alpha_i + \eta \\ b \larr b+\eta y_i$$
4. 转至2直到没有误分类数据

对偶形式中训练实例仅以内积的形式出现。为了方便，可以预先将训练集中实例间的内积计算出来并以矩阵的形式存储，这个矩阵就是所谓的Gram 矩阵(Gram matrix)

$$G=[x_i,x_j]_{N\times N}$$